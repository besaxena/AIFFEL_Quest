{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aedcd203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "import pandas \n",
    "import tensorflow \n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(pandas.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8ea6c",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "## Preprocess Text\n",
    "> Preprocess text\n",
    "> - Convert English text (only) to lower\n",
    "> - Remove the special characters\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b02f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Convert English text to lowercase\n",
    "    if text.isascii():\n",
    "        text = text.lower()\n",
    "\n",
    "    # Remove all characters except Korean, English, numbers, and important special characters\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9!@#$%^&*()_+,\\-.;: ]+\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd5b38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d271c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613178d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(source, target, length = 20):\n",
    "    mecab = Mecab()\n",
    "    source = [preprocess_sentence(s) for s in source]\n",
    "    target = [preprocess_sentence(t) for t in target]\n",
    "    source_tokenized = [mecab.morphs(s) for s in source]\n",
    "    target_tokenized = [mecab.morphs(t) for t in target]\n",
    "    filtered_source = []\n",
    "    filtered_target = []\n",
    "    # what length? -> length\n",
    "    for s, t in zip(source_tokenized, target_tokenized):\n",
    "        if len(s) <= length and len(t) <= length:\n",
    "            # remove duplicates\n",
    "            if s not in filtered_source and t not in filtered_target:\n",
    "                filtered_source.append(s)\n",
    "                filtered_target.append(t)\n",
    "    return filtered_source, filtered_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0ee18",
   "metadata": {},
   "source": [
    "# Step 4. Augmentation\n",
    "다운로드한 모델을 활용해 데이터를 Augmentation 하세요! 앞서 정의한 lexical_sub() 함수를 참고하면 도움이 많이 될 겁니다.\n",
    "\n",
    "Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다.\n",
    "\n",
    "- Lexical substitution \n",
    "    - randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d478cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def lexical_sub(sentence, word_vec):\n",
    "    # 임베딩 모델 로드\n",
    "    model = Word2Vec.load('path/to/word2vec/model')\n",
    "\n",
    "    # 입력 문장을 임베딩으로 확장\n",
    "    embedded_sentence = [model.wv[word] for word in sentence.split()]\n",
    "    \n",
    "    # 대체할 단어 선택\n",
    "    selected_w = random.choice(sentence)\n",
    "\n",
    "    # 대체할 단어와 가장 유사한 단어 찾기\n",
    "    similar_word = model.wv.most_similar(target_word, topn=1)[0][0]\n",
    "\n",
    "    # 선택한 단어로 대체\n",
    "    substituted_sentence = sentence.replace(target_word, similar_word)\n",
    "\n",
    "    return substituted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d29c3e",
   "metadata": {},
   "source": [
    "# Step 6. 훈련하기\n",
    "앞서 번역 모델을 훈련하며 정의한 Transformer 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! 가장 멋진 답변과 모델의 하이퍼파라미터를 제출하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69cb2b",
   "metadata": {},
   "source": [
    "# Step 7. 성능 측정하기\n",
    "- 챗봇의 경우, 올바른 대답을 하는지가 중요한 평가 지표입니다. 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것입니다. 주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수도 적용해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3432c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
